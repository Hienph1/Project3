{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\envs\\audio_recognition\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\Miniconda\\envs\\audio_recognition\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, n_mfcc=10, hop_length=327, win_length=624):\n",
    "   signal, sr = librosa.load(file_path)\n",
    "   mfccs = librosa.feature.mfcc(y=signal,\n",
    "                                 sr=sr,\n",
    "                                 n_mfcc=n_mfcc,\n",
    "                                 hop_length=hop_length,\n",
    "                                 win_length=win_length)\n",
    "   return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "   data = []\n",
    "   labels = []\n",
    "   for root, dirs, files in os.walk(directory):\n",
    "      for file in files:\n",
    "         if file.endswith(\".wav\"):\n",
    "               file_path = os.path.join(root, file)\n",
    "               label = os.path.basename(root)\n",
    "               data.append(extract_features(file_path))\n",
    "               labels.append(label)\n",
    "   data = np.array(data)\n",
    "   labels = np.array(labels)\n",
    "   return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\envs\\audio_recognition\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\Miniconda\\envs\\audio_recognition\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_data(\"data/augmented_data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, labels1 = load_data('data/augmented_data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((data, data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.concatenate((labels, labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['down', 'go', 'left', 'no', 'off', 'on', 'right', 'silence',\n",
       "       'stop', 'unknown', 'up', 'yes'], dtype='<U7')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(labels):\n",
    "   for j, text in enumerate(train_label):\n",
    "      if text == label:\n",
    "         train_label[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {label: i for i, label in enumerate(np.unique(train_label))}\n",
    "encoded_labels = np.array([label_dict[label] for label in train_label])\n",
    "one_hot_labels = to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40206, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, one_hot_labels, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32164, 10, 49)\n",
      "(8042, 10, 49)\n",
      "(32164, 12)\n",
      "(8042, 12)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\matplotlib\\text.py:1223: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 290\n",
    "librosa.display.specshow(X_train[idx], x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(y_train[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 8, 32)             4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 6, 64)             6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 4, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 78,476\n",
      "Trainable params: 78,028\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    # layers.experimental.preprocessing.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    # norm_layer,\n",
    "    layers.Conv1D(32, 3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(64, 3, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(128, 3, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling1D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(num_labels, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "403/403 [==============================] - 5s 10ms/step - loss: 2.3928 - accuracy: 0.2054 - val_loss: 1.9912 - val_accuracy: 0.3552\n",
      "Epoch 2/20\n",
      "403/403 [==============================] - 4s 10ms/step - loss: 1.8849 - accuracy: 0.3377 - val_loss: 1.6392 - val_accuracy: 0.4365\n",
      "Epoch 3/20\n",
      "403/403 [==============================] - 4s 11ms/step - loss: 1.6865 - accuracy: 0.4160 - val_loss: 1.4950 - val_accuracy: 0.4867\n",
      "Epoch 4/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.5551 - accuracy: 0.4621 - val_loss: 1.4115 - val_accuracy: 0.5125\n",
      "Epoch 5/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.5268 - accuracy: 0.4747 - val_loss: 1.3332 - val_accuracy: 0.5497\n",
      "Epoch 6/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.4144 - accuracy: 0.5071 - val_loss: 1.2991 - val_accuracy: 0.5515\n",
      "Epoch 7/20\n",
      "403/403 [==============================] - 3s 8ms/step - loss: 1.3750 - accuracy: 0.5306 - val_loss: 1.2564 - val_accuracy: 0.5677\n",
      "Epoch 8/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.3387 - accuracy: 0.5379 - val_loss: 1.2413 - val_accuracy: 0.5811\n",
      "Epoch 9/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.2839 - accuracy: 0.5585 - val_loss: 1.2245 - val_accuracy: 0.5792\n",
      "Epoch 10/20\n",
      "403/403 [==============================] - 3s 9ms/step - loss: 1.2841 - accuracy: 0.5597 - val_loss: 1.1945 - val_accuracy: 0.5904\n",
      "Epoch 11/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.2278 - accuracy: 0.5763 - val_loss: 1.1821 - val_accuracy: 0.5988\n",
      "Epoch 12/20\n",
      "403/403 [==============================] - 4s 10ms/step - loss: 1.1720 - accuracy: 0.5939 - val_loss: 1.1622 - val_accuracy: 0.6041\n",
      "Epoch 13/20\n",
      "403/403 [==============================] - 4s 11ms/step - loss: 1.2218 - accuracy: 0.5783 - val_loss: 1.1577 - val_accuracy: 0.6103\n",
      "Epoch 14/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.1900 - accuracy: 0.5922 - val_loss: 1.1330 - val_accuracy: 0.6178\n",
      "Epoch 15/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.1339 - accuracy: 0.6088 - val_loss: 1.1338 - val_accuracy: 0.6171\n",
      "Epoch 16/20\n",
      "403/403 [==============================] - 3s 8ms/step - loss: 1.1264 - accuracy: 0.6150 - val_loss: 1.1247 - val_accuracy: 0.6199\n",
      "Epoch 17/20\n",
      "403/403 [==============================] - 3s 9ms/step - loss: 1.1205 - accuracy: 0.6131 - val_loss: 1.1297 - val_accuracy: 0.6206\n",
      "Epoch 18/20\n",
      "403/403 [==============================] - 3s 8ms/step - loss: 1.1034 - accuracy: 0.6172 - val_loss: 1.1156 - val_accuracy: 0.6283\n",
      "Epoch 19/20\n",
      "403/403 [==============================] - 4s 9ms/step - loss: 1.0897 - accuracy: 0.6287 - val_loss: 1.1107 - val_accuracy: 0.6322\n",
      "Epoch 20/20\n",
      "403/403 [==============================] - 3s 9ms/step - loss: 1.0973 - accuracy: 0.6249 - val_loss: 1.1023 - val_accuracy: 0.6272\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size = 64,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=2, patience=4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow dataset from the numpy arrays\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimization flag to use default optimizations\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a representative dataset\n",
    "def representative_data_gen():\n",
    "    for input_value in dataset.batch(1).take(100):\n",
    "        yield [input_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the supported operations to use int8\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the input and output tensors are also int8\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10816\\2128627270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtflite_quant_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m     \"\"\"\n\u001b[1;32m--> 829\u001b[1;33m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;31m# When storing the given keras model to a saved model is failed, let's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   2000\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 2002\u001b[1;33m                     signatures, options, save_traces)\n\u001b[0m\u001b[0;32m   2003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m   def save_weights(self,\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    155\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[1;32m--> 157\u001b[1;33m                           signatures, options, save_traces)\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_traces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0msave_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m   _, exported_graph, object_saver, asset_info = _build_meta_graph(\n\u001b[1;32m-> 1033\u001b[1;33m       obj, signatures, options, meta_graph_def)\n\u001b[0m\u001b[0;32m   1034\u001b[0m   \u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model_schema_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_SCHEMA_VERSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m_build_meta_graph\u001b[1;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0msave_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_build_meta_graph_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m_build_meta_graph_impl\u001b[1;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[0;32m   1145\u001b[0m   \u001b[1;31m# Note we run this twice since, while constructing the view the first time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m   \u001b[1;31m# there can be side effects of creating variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SaveableView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_graph_view\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m   saveable_view = _SaveableView(checkpoint_graph_view, options,\n\u001b[0;32m   1149\u001b[0m                                 wrapped_functions)\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, checkpoint_view, options, wrapped_functions)\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[1;31m#  variables on first run.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m           concrete_functions = (\n\u001b[1;32m--> 225\u001b[1;33m               function._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m           \u001b[0mconcrete_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_list_all_concrete_functions_for_serialization\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[0mconcrete_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseen_signatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m       \u001b[0mconcrete_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1182\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcrete_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save_impl.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayerCall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save_impl.py\u001b[0m in \u001b[0;36madd_trace\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mtrace_with_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save_impl.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayerCall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1297\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mhas\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0myet\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0mconcrete\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \"\"\"\n\u001b[1;32m-> 1299\u001b[1;33m     \u001b[0mconcrete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1215\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m       concrete = self._stateful_fn._get_concrete_function_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m-> 1217\u001b[1;33m           *args, **kwargs)\n\u001b[0m\u001b[0;32m   1218\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    993\u001b[0m       \u001b[1;31m# TensorArrays and `None`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       func_outputs = nest.map_structure(convert, func_outputs,\n\u001b[1;32m--> 995\u001b[1;33m                                         expand_composites=True)\n\u001b[0m\u001b[0;32m    996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m       \u001b[0mcheck_mutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_args_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    952\u001b[0m               (str(python_func), type(x)))\n\u001b[0;32m    953\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36mmark_as_return\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;31m# of a new identity operation that the stateful operations definitely don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;31m# depend on.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_returned_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3940\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3941\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m-> 3942\u001b[1;33m         \"Identity\", input=input, name=name)\n\u001b[0m\u001b[0;32m   3943\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3944\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    748\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    749\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    590\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    591\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3534\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3535\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3536\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3537\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3538\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2014\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2015\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[1;32m-> 2016\u001b[1;33m                                 control_input_ops, op_def)\n\u001b[0m\u001b[0;32m   2017\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Miniconda\\envs\\audio_recognition\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1826\u001b[0m   op_desc = pywrap_tf_session.TF_NewOperation(graph._c_graph,\n\u001b[0;32m   1827\u001b[0m                                               \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1828\u001b[1;33m                                               compat.as_str(node_def.name))\n\u001b[0m\u001b[0;32m   1829\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1830\u001b[0m     \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TensorFlow Lite model\n",
    "with open('models/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = 'models/model.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the file in bytes\n",
    "tflite_model_size_bytes = os.path.getsize(tflite_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite model size: 95240 bytes\n",
      "TensorFlow Lite model size: 93.01 KB\n",
      "TensorFlow Lite model size: 0.09 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the file in bytes\n",
    "tflite_model_size_bytes = os.path.getsize(tflite_model_path)\n",
    "\n",
    "# Convert the size to a more readable format (e.g., kilobytes or megabytes)\n",
    "tflite_model_size_kb = tflite_model_size_bytes / 1024  # Kilobytes\n",
    "tflite_model_size_mb = tflite_model_size_bytes / (1024 * 1024)  # Megabytes\n",
    "\n",
    "# Print the size\n",
    "print(f\"TensorFlow Lite model size: {tflite_model_size_bytes} bytes\")\n",
    "print(f\"TensorFlow Lite model size: {tflite_model_size_kb:.2f} KB\")\n",
    "print(f\"TensorFlow Lite model size: {tflite_model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\phamh\\AppData\\Local\\Temp\\tmpkckx6mut\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\phamh\\AppData\\Local\\Temp\\tmpkckx6mut\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is 317656 bytes\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"models/use_library_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "basic_model_size = os.path.getsize(\"models/use_library_model.tflite\")\n",
    "print(\"Model is %d bytes\" % basic_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"models\\model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the quantization parameters for the input\n",
    "input_scale, input_zero_point = input_details[0]['quantization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for predictions\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "   input_data = X_test[i:i+1]  # Get a single test sample\n",
    "\n",
    "   # Quantize the input data from float32 to uint8\n",
    "   input_data = (input_data / input_scale + input_zero_point).astype(np.uint8)\n",
    "\n",
    "   # Set input tensor\n",
    "   interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "   # Run inference\n",
    "   interpreter.invoke()\n",
    "\n",
    "   # Get output tensor\n",
    "   output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "   predictions.append(output_data)\n",
    "\n",
    "predictions = np.array(predictions).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1) if predictions.ndim > 1 else np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.20%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_predict = model.predict(X_test)\n",
    "org_pred = np.argmax(org_predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.78%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true, org_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
